{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove #OTUID string (ONLY EXECUTE ONCE!)\n",
    "!find output/table_out/*.txt -type f | while read FILENAME ; do sed -i.bak -E '1s/^.{8}//' \"${FILENAME}\"; done \n",
    "!rm output/table_out/*.bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened files: ['FMTT11_97.txt', 'FMTT12_97.txt', 'FMTT13_97.txt', 'FMTT14_97.txt', 'FMTT15_97.txt', 'FMTT16_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 53441\n",
      "merged dataframe is 53441 columns and 3142 rows\n",
      "output dataframe is 53444 columns and 5951 rows\n",
      "\n",
      "opened files: ['FMTT21_97.txt', 'FMTT22_97.txt', 'FMTT23_97.txt', 'FMTT24_97.txt', 'FMTT25_97.txt', 'FMTT26_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 51135\n",
      "merged dataframe is 51135 columns and 3334 rows\n",
      "output dataframe is 51138 columns and 5951 rows\n",
      "\n",
      "opened files: ['FMTT31_97.txt', 'FMTT32_97.txt', 'FMTT33_97.txt', 'FMTT34_97.txt', 'FMTT35_97.txt', 'FMTT36_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 37804\n",
      "merged dataframe is 37804 columns and 3048 rows\n",
      "output dataframe is 37807 columns and 5951 rows\n",
      "\n",
      "opened files: ['FMTT41_97.txt', 'FMTT42_97.txt', 'FMTT43_97.txt', 'FMTT44_97.txt', 'FMTT45_97.txt', 'FMTT46_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 50531\n",
      "merged dataframe is 50531 columns and 3219 rows\n",
      "output dataframe is 50534 columns and 5951 rows\n",
      "\n",
      "opened files: ['M10CL1_97.txt', 'M10CL2_97.txt', 'M10CL3_97.txt', 'M10CL4_97.txt', 'M10CL5_97.txt', 'M10CL6_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 48698\n",
      "merged dataframe is 48698 columns and 3835 rows\n",
      "output dataframe is 48701 columns and 5951 rows\n",
      "\n",
      "opened files: ['M10CS1_97.txt', 'M10CS2_97.txt', 'M10CS3_97.txt', 'M10CS4_97.txt', 'M10CS5_97.txt', 'M10CS6_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 9144\n",
      "merged dataframe is 9144 columns and 1118 rows\n",
      "output dataframe is 9147 columns and 5951 rows\n",
      "\n",
      "opened files: ['M10SL1_97.txt', 'M10SL2_97.txt', 'M10SL3_97.txt', 'M10SL4_97.txt', 'M10SL5_97.txt', 'M10SL6_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 48721\n",
      "merged dataframe is 48721 columns and 705 rows\n",
      "output dataframe is 48724 columns and 5951 rows\n",
      "\n",
      "opened files: ['M10SS1_97.txt', 'M10SS2_97.txt', 'M10SS3_97.txt', 'M10SS4_97.txt', 'M10SS5_97.txt', 'M10SS6_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 13321\n",
      "merged dataframe is 13321 columns and 413 rows\n",
      "output dataframe is 13324 columns and 5951 rows\n",
      "\n",
      "opened files: ['M8C1_97.txt', 'M8C2_97.txt', 'M8C3_97.txt', 'M8C4_97.txt', 'M8C5_97.txt', 'M8C6_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 43516\n",
      "merged dataframe is 43516 columns and 1122 rows\n",
      "output dataframe is 43519 columns and 5951 rows\n",
      "\n",
      "opened files: ['M9C1_97.txt', 'M9C2_97.txt', 'M9C3_97.txt', 'M9C4_97.txt', 'M9C5_97.txt', 'M9C6_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 46172\n",
      "merged dataframe is 46172 columns and 1829 rows\n",
      "output dataframe is 46175 columns and 5951 rows\n",
      "\n",
      "opened files: ['TRM10CL1_97.txt', 'TRM10CL2_97.txt', 'TRM10CL3_97.txt', 'TRM10CL4_97.txt', 'TRM10CL5_97.txt', 'TRM10CL6_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 30139\n",
      "merged dataframe is 30139 columns and 3419 rows\n",
      "output dataframe is 30142 columns and 5951 rows\n",
      "\n",
      "opened files: ['TRM10SL1_97.txt', 'TRM10SL2_97.txt', 'TRM10SL3_97.txt', 'TRM10SL4_97.txt', 'TRM10SL5_97.txt', 'TRM10SL6_97.txt']\n",
      "read 6 data frames from directory\n",
      "the number of columns summed from individual dataframes is 50798\n",
      "merged dataframe is 50798 columns and 772 rows\n",
      "output dataframe is 50801 columns and 5951 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path='output/table_out/'\n",
    "files = sorted([f for f in listdir(path) if isfile(join(path, f))])\n",
    "sample_names = [files[i].split('_')[0][:-1] for i in np.arange(0,72,6)]\n",
    "for i_sn in range(len(sample_names)):\n",
    "    files_subset=files[6*i_sn:6*i_sn+6]\n",
    "    df_arr=[]\n",
    "    for f in files_subset:\n",
    "        df_arr.append(pd.read_csv(path+f, delim_whitespace=True, header=0,index_col=0))\n",
    "    print 'opened files: '+str(files_subset)\n",
    "    print 'read '+str(len(df_arr))+' data frames from directory'\n",
    "    #merge all dataframes together\n",
    "    merge_df=pd.merge(df_arr[0],df_arr[1],left_index=True,right_index=True,how='outer')\n",
    "    for i in range(2,len(df_arr)):\n",
    "        merge_df=pd.merge(merge_df,df_arr[i],left_index=True,right_index=True,how='outer')\n",
    "    merge_df=merge_df.fillna(0) #make all nans 0\n",
    "    #count number of columns \n",
    "    col=0\n",
    "    for data_frame in df_arr:\n",
    "        col=col+data_frame.shape[1]\n",
    "    print 'the number of columns summed from individual dataframes is '+str(col)\n",
    "    print 'merged dataframe is '+str(merge_df.shape[1])+' columns and '+str(merge_df.shape[0])+' rows'\n",
    "    \n",
    "    #read in RDP taxonomy and assign to rdp_taxonomy field\n",
    "    merge_df.index = [int(rec.split('Otu')[1]) for rec in merge_df.index.values]\n",
    "    merge_df=merge_df.sort_index(axis=0)\n",
    "    rdp_fix=np.loadtxt('output/allrank_otus.fa_classified.txt',dtype=str,delimiter='\\n')\n",
    "    rdp_fix=rdp_fix[6:]\n",
    "    otu_index=[int(i.split(';',1)[0].split('Otu')[1]) for i in rdp_fix]\n",
    "    rdp_fix=[i.split(';',1)[1].split(';',1)[1] for i in rdp_fix]\n",
    "    rdp_all=np.loadtxt('output/fixrank_otus.fa_classified.txt',dtype=str,delimiter='\\n')\n",
    "    rdp_all=rdp_all[6:]\n",
    "    rdp_all=[i.split(';',1)[1].split(';',1)[1] for i in rdp_all]\n",
    "\n",
    "    #read in sequences\n",
    "    def fasta_iter(fasta_name): #iterator for fasta files\n",
    "        fh = open(fasta_name)\n",
    "        faiter = (x[1] for x in groupby(fh, lambda line: line[0] == \">\"))\n",
    "        for header in faiter:\n",
    "            header = header.next()[1:].strip()\n",
    "            seq = \"\".join(s.strip() for s in faiter.next())\n",
    "            yield header, seq\n",
    "    seq=[]\n",
    "    fiter=fasta_iter('output/otus.fa') #open fasta file via iterator\n",
    "    for rec in fiter: #iterate through records\n",
    "        h,s = rec #extract header and sequence\n",
    "        seq.append(s) #extract sequence\n",
    "\n",
    "    info_df = pd.DataFrame(index=otu_index)\n",
    "    info_df['rdp_fix_taxonomy']=rdp_fix\n",
    "    info_df['rdp_all_taxonomy']=rdp_all\n",
    "    info_df['sequence']=seq\n",
    "    merge_df=pd.merge(merge_df.astype(int),info_df,left_index=True,right_index=True,how='outer')\n",
    "    merge_df=merge_df.fillna(0) #make all nans 0\n",
    "    merge_df=merge_df.sort_index(axis=0)\n",
    "    print 'output dataframe is '+str(merge_df.shape[1])+' columns and '+str(merge_df.shape[0])+' rows'\n",
    "    merge_df.to_hdf('output/'+sample_names[i_sn]+'.h5','table',mode='w',complevel=9,complib='bzip2')\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=merge_df\n",
    "plt.figure(figsize=(6, 10))\n",
    "regex=['^FMTT11','^FMTT12','^FMTT13','^FMTT14','^FMTT15','^FMTT16']\n",
    "cnt=1\n",
    "for r in regex:\n",
    "    plt.subplot(6,2,cnt)\n",
    "    plt.hist(np.log10(df.filter(regex=r).sum(axis=0)), bins=25, color='k')\n",
    "    plt.xlabel('log10(#reads)')\n",
    "    plt.ylabel('#particles')\n",
    "    plt.ylim([0,500])\n",
    "    cnt+=1\n",
    "    \n",
    "    plt.subplot(6,2,cnt)\n",
    "    fil=df.filter(regex=r)\n",
    "    ordered=np.sort(np.array(fil.sum(axis=0)))[::-1]\n",
    "    cum=np.zeros(ordered.shape)\n",
    "    for i in range(ordered.shape[0]):\n",
    "        cum[i]=ordered[i]\n",
    "    plt.plot(range(cum.shape[0]), cum,'k-')\n",
    "    plt.xlabel('particles, ordered largest to smallest')\n",
    "    plt.ylabel('# reads per particle')\n",
    "    plt.ylim([0,500])\n",
    "    plt.xlim([0,1000])\n",
    "    cnt+=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pres=df.filter(regex='FMTT16').T.loc[df.filter(regex='FMTT16').sum(axis=0)>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io, scipy.sparse\n",
    "scipy.io.mmwrite(\"mmout\", scipy.sparse.csr_matrix(merge_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_dad=scipy.sparse.csr_matrix(merge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
